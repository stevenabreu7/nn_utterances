{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import optparse\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    \"\"\"Logging in tensorboard without tensorflow ops.\"\"\"\n",
    "\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\n",
    "        Parameter\n",
    "        ----------\n",
    "        tag : Name of the scalar\n",
    "        value : value itself\n",
    "        step :  training iteration\n",
    "        \"\"\"\n",
    "        # Notice we're using the Summary \"class\" instead of the \"tf.summary\" public API.\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def log_histogram(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Logs the histogram of a list/vector of values.\"\"\"\n",
    "        # Convert to a numpy array\n",
    "        values = np.array(values)\n",
    "        \n",
    "        # Create histogram using numpy        \n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill fields of histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Requires equal number as bins, where the first goes from -DBL_MAX to bin_edges[1]\n",
    "        # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/summary.proto#L30\n",
    "        # Thus, we drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_array_temporally(X, padding):\n",
    "    right_shift = lambda X, i : np.pad(X[:-i], [(i,0),(0,0)], 'constant', constant_values=0)\n",
    "    left_shift = lambda X, i : np.pad(X[i:], [(0,i),(0,0)], 'constant', constant_values=0)\n",
    "    before = [right_shift(X, i) for i in range(padding, 0, -1)]\n",
    "    rest = [left_shift(X,i) for i in range(0, padding+1)]\n",
    "    return np.concatenate(before + rest, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data():\n",
    "    # Getting the labels\n",
    "    trainy = np.load('data/train_labels.npy', encoding='bytes')\n",
    "    valy = np.load('data/dev_labels.npy', encoding='bytes')\n",
    "    trainy = np.concatenate(trainy.tolist())\n",
    "    valy = np.concatenate(valy.tolist())\n",
    "    # PCA\n",
    "    if os.path.exists('data/trainx_pca.npy'):\n",
    "        trainx = np.load('data/trainx_pca.npy')\n",
    "        testx = np.load('data/testx_pca.npy')\n",
    "        valx = np.load('data/valx_pca.npy')\n",
    "    else:\n",
    "        trainx = np.load('data/train.npy', encoding='bytes')\n",
    "        testx = np.load('data/test.npy', encoding='bytes')\n",
    "        valx = np.load('data/dev.npy', encoding='bytes')\n",
    "        trainx = np.concatenate(trainx.tolist())\n",
    "        testx = np.concatenate(testx.tolist())\n",
    "        valx = np.concatenate(valx.tolist())\n",
    "        trainx = PCA(n_components=10).fit_transform(trainx)\n",
    "        testx = PCA(n_components=10).fit_transform(testx)\n",
    "        valx = PCA(n_components=10).fit_transform(valx)\n",
    "        np.save('data/trainx_pca.npy', trainxs)\n",
    "        np.save('data/testx_pca.npy', testxs)\n",
    "        np.save('data/valx_pca.npy', valxs)\n",
    "    # add context\n",
    "    padding = 20\n",
    "    trainx = pad_array_temporally(trainx, padding)\n",
    "    valx = pad_array_temporally(valx, padding)\n",
    "    # Turn into tensors\n",
    "    trainx = torch.from_numpy(trainx).float()\n",
    "    trainy = torch.from_numpy(trainy.astype(int))\n",
    "    testx = torch.from_numpy(testx).float()\n",
    "    valx = torch.from_numpy(valx).float()\n",
    "    valy = torch.from_numpy(valy.astype(int))\n",
    "    # return\n",
    "    return trainx, trainy, valx, valy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainx, trainy, valx, valy = load_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_routine(name, net, dataset, epochs, lr, batch_size=5000, decay=True, logging=False):\n",
    "\n",
    "    if logging:\n",
    "        vLog = Logger('./logs/val_acc_{}'.format(name))\n",
    "        tLog = Logger('./logs/train_acc_{}'.format(name))\n",
    "    \n",
    "    train_data, train_labels, val_data, val_labels = dataset\n",
    "    \n",
    "    criterion=nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    if decay:\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.3)\n",
    "    \n",
    "    gpu = torch.cuda.is_available()\n",
    "    print('Using GPU' if gpu else 'Not using GPU')\n",
    "    \n",
    "    net = net.cuda() if gpu else net\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        train_correct = 0\n",
    "        train_loss = []\n",
    "\n",
    "        for batch_n in range(0, train_data.shape[0], batch_size):\n",
    "            \n",
    "            if (batch_n // batch_size) % 10 == 0:\n",
    "                print('\\rEpoch {:4} Batch {:6} ({:.2%})'.format(epoch + 1, batch_n // batch_size, batch_n / train_data.shape[0]), end='')\n",
    "            \n",
    "            a, b = batch_n, batch_n + batch_size\n",
    "            batch_data, batch_labels = train_data[a:b], train_labels[a:b]\n",
    "\n",
    "            if gpu:\n",
    "                batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            batch_output = net(batch_data)\n",
    "            batch_loss = criterion(batch_output, batch_labels)\n",
    "            \n",
    "            # backward pass\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            batch_prediction = batch_output.cpu().detach().argmax(dim=1)\n",
    "            batch_correct = (batch_prediction.numpy() == batch_labels.cpu().detach().numpy()).sum()\n",
    "            \n",
    "            train_correct += batch_correct\n",
    "            \n",
    "            if batch_loss:\n",
    "                train_loss.append(batch_loss.cpu().detach())\n",
    "        \n",
    "        train_accuracy = train_correct / train_data.shape[0]\n",
    "        train_loss = torch.FloatTensor(train_loss)\n",
    "        \n",
    "        if logging:\n",
    "            tLog.log_scalar('accuracy', train_accuracy, epoch + 1)\n",
    "            for tag, value in net.named_parameters():\n",
    "                tag = tag.replace('.', '/')\n",
    "                tLog.log_histogram(tag, value.data.cpu().numpy(), epoch + 1)\n",
    "                tLog.log_histogram(tag + '/grad', value.grad.data.cpu().numpy(), epoch + 1)\n",
    "        \n",
    "        # Once every 10 iterations, print statistics\n",
    "        if True: #if epoch == 0 or epoch+1 % 10 == 0:\n",
    "            \n",
    "            print(\"\\rStatistics for epoch\", epoch + 1)\n",
    "            \n",
    "            val_loss = 0\n",
    "            val_correct = 0\n",
    "            count = 0\n",
    "            \n",
    "            for batch_n in range(0, val_data.shape[0], batch_size):\n",
    "                # create batches\n",
    "                a, b = batch_n, batch_n + batch_size\n",
    "                batch_data = val_data[a:b]\n",
    "                batch_labels = val_labels[a:b]\n",
    "                \n",
    "                # use GPU if possible \n",
    "                if gpu:\n",
    "                    batch_data, batch_labels = batch_data.cuda(), batch_labels.cuda()\n",
    "                \n",
    "                # Now for the validation set\n",
    "                batch_output = net(batch_data)\n",
    "                batch_loss = criterion(batch_output, batch_labels)\n",
    "                \n",
    "                # compute the accuracy of the prediction\n",
    "                val_prediction = batch_output.cpu().detach().argmax(dim=1)\n",
    "                val_correct += (val_prediction.numpy() == batch_labels.cpu().detach().numpy()).sum()\n",
    "                \n",
    "                # sum up to get mean later\n",
    "                val_loss += val_loss\n",
    "                count += 1\n",
    "                \n",
    "            # compute mean validation loss and accuracy for all batches\n",
    "            val_loss = val_loss / count\n",
    "            val_accuracy = val_correct / val_data.shape[0]\n",
    "                \n",
    "            print(\"Training loss :\",train_loss.cpu().detach().numpy())\n",
    "            print(\"Training accuracy :\",train_accuracy)\n",
    "            print(\"Validation loss :\",val_loss)\n",
    "            print(\"Validation accuracy :\",val_accuracy)\n",
    "            print()\n",
    "            \n",
    "            if logging:\n",
    "                vLog.log_scalar('accuracy', val_accuracy, epoch + 1) \n",
    "                for tag, value in net.named_parameters():\n",
    "                    tag = tag.replace('.', '/')\n",
    "                    tLog.log_histogram(tag, value.data.cpu().numpy(), epoch + 1)\n",
    "                    tLog.log_histogram(tag + '/grad', value.grad.data.cpu().numpy(), epoch + 1)\n",
    "\n",
    "            # try to save the neural network\n",
    "            try:\n",
    "                torch.save(net, 'neural_net_{}'.format(name))\n",
    "            except:\n",
    "                print('Could not save neural network.')\n",
    "\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_net(layers, batch_norm=True):\n",
    "    model = []\n",
    "    for i in range(len(layers)-1):\n",
    "        model.append(nn.Linear(layers[i], layers[i+1]))\n",
    "        if i < (len(layers)-2):\n",
    "            if batch_norm:\n",
    "                model.append(nn.BatchNorm1d(layers[i+1]))\n",
    "            model.append(nn.LeakyReLU())\n",
    "    net = nn.Sequential(*tuple(model))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Statistics for epoch 1\n",
      "Training loss : [4.95939   4.767304  4.6081405 ... 1.4083661 1.3654275 1.4095609]\n",
      "Training accuracy : 0.4309052817069839\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.3773842287544786\n",
      "\n",
      "Statistics for epoch 2\n",
      "Training loss : [2.4697251 2.3843856 1.9720542 ... 1.2989067 1.27318   1.302018 ]\n",
      "Training accuracy : 0.4980918418317179\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.3982136400445843\n",
      "\n",
      "Statistics for epoch 3\n",
      "Training loss : [2.3138921 2.1974685 1.8101026 ... 1.2426203 1.2258229 1.2467799]\n",
      "Training accuracy : 0.5173215866125288\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4086948336605438\n",
      "\n",
      "Statistics for epoch 4\n",
      "Training loss : [2.2237425 2.0920994 1.7310332 ... 1.2111076 1.1970118 1.2112907]\n",
      "Training accuracy : 0.5286180357275666\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.41660017869576\n",
      "\n",
      "Statistics for epoch 5\n",
      "Training loss : [2.1533859 2.0199068 1.6712006 ... 1.1848785 1.1710526 1.1881064]\n",
      "Training accuracy : 0.5365221389262389\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.42324299933960263\n",
      "\n",
      "Statistics for epoch 6\n",
      "Training loss : [2.0932539 2.0019884 1.6737113 ... 1.164093  1.1645468 1.1744841]\n",
      "Training accuracy : 0.5437623885936811\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4427366747647521\n",
      "\n",
      "Statistics for epoch 7\n",
      "Training loss : [1.9780995 1.8830478 1.5612336 ... 1.1575854 1.1593876 1.1676979]\n",
      "Training accuracy : 0.5476887430545716\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4448015371421229\n",
      "\n",
      "Statistics for epoch 8\n",
      "Training loss : [1.9600438 1.8640293 1.5405744 ... 1.1499076 1.152996  1.161286 ]\n",
      "Training accuracy : 0.5500651134418624\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.44641517778435186\n",
      "\n",
      "Statistics for epoch 9\n",
      "Training loss : [1.9466146 1.8492007 1.5248592 ... 1.1415707 1.1476269 1.1549368]\n",
      "Training accuracy : 0.5520111700347287\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.44758357313826214\n",
      "\n",
      "Statistics for epoch 10\n",
      "Training loss : [1.9363151 1.8358959 1.5111393 ... 1.134768  1.1423633 1.149094 ]\n",
      "Training accuracy : 0.5537734629599699\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4484665931563707\n",
      "\n",
      "Statistics for epoch 11\n",
      "Training loss : [1.9252214 1.8365276 1.519034  ... 1.1427741 1.1505121 1.1592963]\n",
      "Training accuracy : 0.5522049018618516\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4590015747937379\n",
      "\n",
      "Statistics for epoch 12\n",
      "Training loss : [1.8697008 1.7841372 1.4663951 ... 1.1433076 1.15088   1.1601985]\n",
      "Training accuracy : 0.5538635647653006\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.45978598343926586\n",
      "\n",
      "Statistics for epoch 13\n",
      "Training loss : [1.8635192 1.7767781 1.4580996 ... 1.1418316 1.1497116 1.159652 ]\n",
      "Training accuracy : 0.554728464422506\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.46033880477039985\n",
      "\n",
      "Statistics for epoch 14\n",
      "Training loss : [1.8587811 1.7715375 1.4519591 ... 1.1400487 1.1481037 1.1583691]\n",
      "Training accuracy : 0.5554899282428446\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.46086921442594736\n",
      "\n",
      "Statistics for epoch 15\n",
      "Training loss : [1.8546115 1.7666113 1.4469128 ... 1.1379752 1.1465434 1.1568905]\n",
      "Training accuracy : 0.556163361563722\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4612771069216219\n",
      "\n",
      "Statistics for epoch 16\n",
      "Training loss : [1.8506832 1.7659072 1.4483656 ... 1.1529902 1.1620773 1.169593 ]\n",
      "Training accuracy : 0.5532247610894317\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4674358353727958\n",
      "\n",
      "Statistics for epoch 17\n",
      "Training loss : [1.8223568 1.7417558 1.4196625 ... 1.1554797 1.1639441 1.1717427]\n",
      "Training accuracy : 0.5541746490156022\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.46798118614540096\n",
      "\n",
      "Statistics for epoch 18\n",
      "Training loss : [1.8194828 1.7389529 1.4161968 ... 1.1560568 1.1642687 1.172288 ]\n",
      "Training accuracy : 0.5545201039976786\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4681784088905623\n",
      "\n",
      "Statistics for epoch 19\n",
      "Training loss : [1.8178512 1.736861  1.4140179 ... 1.1560272 1.1640894 1.1722943]\n",
      "Training accuracy : 0.5548484707063301\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4685175722477715\n",
      "\n",
      "Statistics for epoch 20\n",
      "Training loss : [1.8164212 1.7350934 1.4122722 ... 1.1557175 1.1637149 1.1721176]\n",
      "Training accuracy : 0.5551021409470567\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4687342184451078\n",
      "\n",
      "Statistics for epoch 21\n",
      "Training loss : [1.8151356 1.7345209 1.4122455 ... 1.1680344 1.1804035 1.1808624]\n",
      "Training accuracy : 0.5529220267909174\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47149981921248363\n",
      "\n",
      "Statistics for epoch 22\n",
      "Training loss : [1.8053379 1.7260934 1.3998564 ... 1.1702121 1.1821718 1.1827068]\n",
      "Training accuracy : 0.5534783018735415\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47176128876099294\n",
      "\n",
      "Statistics for epoch 23\n",
      "Training loss : [1.8037306 1.724708  1.3982606 ... 1.1711228 1.1828097 1.1834506]\n",
      "Training accuracy : 0.5536417408523203\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.471915182266687\n",
      "\n",
      "Statistics for epoch 24\n",
      "Training loss : [1.802835  1.7238827 1.3972743 ... 1.1715955 1.1830432 1.1838144]\n",
      "Training accuracy : 0.5537719094805676\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47200931130415036\n",
      "\n",
      "Statistics for epoch 25\n",
      "Training loss : [1.8022035 1.7232242 1.3965018 ... 1.171847  1.1831074 1.184012 ]\n",
      "Training accuracy : 0.5538726914567889\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4720899933362618\n",
      "\n",
      "Statistics for epoch 26\n",
      "Training loss : [1.8016899 1.7228906 1.3961802 ... 1.1805605 1.1923523 1.1915646]\n",
      "Training accuracy : 0.5528303067778759\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47361249316443893\n",
      "\n",
      "Statistics for epoch 27\n",
      "Training loss : [1.7967309 1.7188524 1.3923861 ... 1.1823928 1.1939778 1.1931511]\n",
      "Training accuracy : 0.5531085737758048\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47374098677113496\n",
      "\n",
      "Statistics for epoch 28\n",
      "Training loss : [1.7961766 1.7182963 1.391714  ... 1.183177  1.194632  1.1938066]\n",
      "Training accuracy : 0.5531747261070176\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4738037394627772\n",
      "\n",
      "Statistics for epoch 29\n",
      "Training loss : [1.7957798 1.7179418 1.3913003 ... 1.1836208 1.194991  1.1941638]\n",
      "Training accuracy : 0.553218417715206\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47385155103736176\n",
      "\n",
      "Statistics for epoch 30\n",
      "Training loss : [1.7954514 1.7176825 1.3909782 ... 1.1839068 1.1952022 1.1943879]\n",
      "Training accuracy : 0.5532592612778235\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4738679862661252\n",
      "\n",
      "Statistics for epoch 31\n",
      "Training loss : [1.7951795 1.7175158 1.390769  ... 1.1876376 1.1989206 1.1989902]\n",
      "Training accuracy : 0.5529163954280842\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4744880426240187\n",
      "\n",
      "Statistics for epoch 32\n",
      "Training loss : [1.7921035 1.7154496 1.3873045 ... 1.1889726 1.2000602 1.2004354]\n",
      "Training accuracy : 0.5530635228731394\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4746688301404166\n",
      "\n",
      "Statistics for epoch 33\n",
      "Training loss : [1.7915549 1.7151524 1.3868039 ... 1.1896443 1.2006168 1.2010773]\n",
      "Training accuracy : 0.5530869545207902\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.474743535725705\n",
      "\n",
      "Statistics for epoch 34\n",
      "Training loss : [1.7913525 1.7150104 1.3866185 ... 1.1900473 1.2009511 1.2014341]\n",
      "Training accuracy : 0.5531090268739638\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4747614650661742\n",
      "\n",
      "Statistics for epoch 35\n",
      "Training loss : [1.7912252 1.7148986 1.3865047 ... 1.1903174 1.201175  1.2016612]\n",
      "Training accuracy : 0.5531255325926128\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4747898531885838\n",
      "\n",
      "Statistics for epoch 36\n",
      "Training loss : [1.791124  1.7148149 1.3864266 ... 1.1911154 1.2020569 1.202854 ]\n",
      "Training accuracy : 0.5530571794989135\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4746748065872397\n",
      "\n",
      "Statistics for epoch 37\n",
      "Training loss : [1.790584  1.7145072 1.3854169 ... 1.1916277 1.2025791 1.2035117]\n",
      "Training accuracy : 0.5530984761596902\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47467630069894545\n",
      "\n",
      "Statistics for epoch 38\n",
      "Training loss : [1.79027   1.7143393 1.3849914 ... 1.1919613 1.2028992 1.2039211]\n",
      "Training accuracy : 0.5531160175312739\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.474659865470182\n",
      "\n",
      "Statistics for epoch 39\n",
      "Training loss : [1.7900895 1.7142477 1.3847655 ... 1.192205  1.203124  1.2042009]\n",
      "Training accuracy : 0.5531195775882375\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4746479125765359\n",
      "\n",
      "Statistics for epoch 40\n",
      "Training loss : [1.7899779 1.7141906 1.3846323 ... 1.1923951 1.2032938 1.204403 ]\n",
      "Training accuracy : 0.55312482058122\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47466135958188777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'id1'\n",
    "layers = [410, 400, 400, 400, 138]\n",
    "batch_norm = True\n",
    "epochs = 40\n",
    "lrate = 3e-1\n",
    "batch_size = 5000\n",
    "decay = True\n",
    "\n",
    "net = generate_net(layers, batch_norm)\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, lrate, batch_size=batch_size, decay=True, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Statistics for epoch 1\n",
      "Training loss : [4.963709  4.828221  4.6629767 ... 1.3878098 1.3414114 1.3750061]\n",
      "Training accuracy : 0.43582884048750514\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.37997501845227954\n",
      "\n",
      "Statistics for epoch 2\n",
      "Training loss : [2.4626937 2.321765  1.9038656 ... 1.2710763 1.2457176 1.2751831]\n",
      "Training accuracy : 0.5052981091372357\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4007386888273315\n",
      "\n",
      "Statistics for epoch 3\n",
      "Training loss : [2.287515  2.1489248 1.7500489 ... 1.2096221 1.1967684 1.2189827]\n",
      "Training accuracy : 0.5251892477735566\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4127199705958816\n",
      "\n",
      "Statistics for epoch 4\n",
      "Training loss : [2.1888752 2.0498128 1.6771473 ... 1.1685017 1.1617665 1.1794478]\n",
      "Training accuracy : 0.537032068540029\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.42158752356961215\n",
      "\n",
      "Statistics for epoch 5\n",
      "Training loss : [2.1127245 1.9751824 1.6209257 ... 1.1409633 1.1362598 1.1508292]\n",
      "Training accuracy : 0.5453615661816855\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.42766407587696886\n",
      "\n",
      "Statistics for epoch 6\n",
      "Training loss : [2.0640776 1.9561336 1.6165094 ... 1.1219201 1.1179844 1.129127 ]\n",
      "Training accuracy : 0.5535060055895483\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4486249689971821\n",
      "\n",
      "Statistics for epoch 7\n",
      "Training loss : [1.9452485 1.8278108 1.5061415 ... 1.1181058 1.1126541 1.1218942]\n",
      "Training accuracy : 0.5577281684199515\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.45062259634779334\n",
      "\n",
      "Statistics for epoch 8\n",
      "Training loss : [1.9231104 1.8043059 1.4864727 ... 1.1124234 1.105872  1.114837 ]\n",
      "Training accuracy : 0.5602782695870612\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4521764725217916\n",
      "\n",
      "Statistics for epoch 9\n",
      "Training loss : [1.9067215 1.7871537 1.4699348 ... 1.1059816 1.0991098 1.1076992]\n",
      "Training accuracy : 0.5624372175863448\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.453016163300433\n",
      "\n",
      "Statistics for epoch 10\n",
      "Training loss : [1.8931818 1.7708808 1.4562179 ... 1.0999683 1.0927795 1.1019589]\n",
      "Training accuracy : 0.5643694223212077\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.45421294677675284\n",
      "\n",
      "Statistics for epoch 11\n",
      "Training loss : [1.8802482 1.7677209 1.4587216 ... 1.1069409 1.1000305 1.107303 ]\n",
      "Training accuracy : 0.5630309056312398\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4650437625318619\n",
      "\n",
      "Statistics for epoch 12\n",
      "Training loss : [1.8315172 1.7073901 1.4057742 ... 1.107652  1.1003294 1.1076239]\n",
      "Training accuracy : 0.5649197423994564\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.46604182915131465\n",
      "\n",
      "Statistics for epoch 13\n",
      "Training loss : [1.8236334 1.6993821 1.3974495 ... 1.1059592 1.0992337 1.1061562]\n",
      "Training accuracy : 0.5659025770346162\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.46665590906238513\n",
      "\n",
      "Statistics for epoch 14\n",
      "Training loss : [1.8178124 1.6932915 1.3915031 ... 1.1041676 1.0978655 1.1044964]\n",
      "Training accuracy : 0.5667448865121805\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4671594247072288\n",
      "\n",
      "Statistics for epoch 15\n",
      "Training loss : [1.8125998 1.6880647 1.3868366 ... 1.1022038 1.0962021 1.1026393]\n",
      "Training accuracy : 0.5675147650126146\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.46760915233066486\n",
      "\n",
      "Statistics for epoch 16\n",
      "Training loss : [1.8079983 1.6866826 1.3865911 ... 1.1204796 1.111436  1.1131861]\n",
      "Training accuracy : 0.5644386816112248\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4731836831048837\n",
      "\n",
      "Statistics for epoch 17\n",
      "Training loss : [1.7848959 1.6622459 1.3579935 ... 1.1224899 1.1130996 1.1154189]\n",
      "Training accuracy : 0.5655177025127076\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4738216688032464\n",
      "\n",
      "Statistics for epoch 18\n",
      "Training loss : [1.7813148 1.658633  1.3541217 ... 1.1226804 1.113182  1.1157477]\n",
      "Training accuracy : 0.5659325462414181\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4742310554106267\n",
      "\n",
      "Statistics for epoch 19\n",
      "Training loss : [1.7791209 1.6562469 1.3517907 ... 1.1223514 1.1129625 1.1155506]\n",
      "Training accuracy : 0.5662668032261365\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47445068983137456\n",
      "\n",
      "Statistics for epoch 20\n",
      "Training loss : [1.777474  1.6541895 1.3498085 ... 1.1217788 1.1125364 1.1150563]\n",
      "Training accuracy : 0.5665569155045076\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47463147734777245\n",
      "\n",
      "Statistics for epoch 21\n",
      "Training loss : [1.7759432 1.6533169 1.3492892 ... 1.1390247 1.1308134 1.1261898]\n",
      "Training accuracy : 0.5642107732372523\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47751063060478655\n",
      "\n",
      "Statistics for epoch 22\n",
      "Training loss : [1.7664104 1.6475164 1.33946   ... 1.1412673 1.1326077 1.1282634]\n",
      "Training accuracy : 0.5648414858745678\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47783186462152655\n",
      "\n",
      "Statistics for epoch 23\n",
      "Training loss : [1.7642214 1.6458948 1.3376826 ... 1.1420485 1.1332006 1.1290789]\n",
      "Training accuracy : 0.5650192298095091\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47810976939879934\n",
      "\n",
      "Statistics for epoch 24\n",
      "Training loss : [1.7630256 1.6449177 1.3366362 ... 1.1423401 1.1334069 1.1293902]\n",
      "Training accuracy : 0.5651553534421317\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47816953386703004\n",
      "\n",
      "Statistics for epoch 25\n",
      "Training loss : [1.7622184 1.6441348 1.3358119 ... 1.1424236 1.1334503 1.1294727]\n",
      "Training accuracy : 0.565282803481425\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4783234273727241\n",
      "\n",
      "Statistics for epoch 26\n",
      "Training loss : [1.7615832 1.6436957 1.3353943 ... 1.1530317 1.1448863 1.1396505]\n",
      "Training accuracy : 0.5641090850647131\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.479551587194865\n",
      "\n",
      "Statistics for epoch 27\n",
      "Training loss : [1.7585795 1.6424252 1.3324206 ... 1.1551019 1.1465678 1.1415278]\n",
      "Training accuracy : 0.5644401056340103\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.47974880994002633\n",
      "\n",
      "Statistics for epoch 28\n",
      "Training loss : [1.7577506 1.642016  1.3315653 ... 1.1559545 1.1472087 1.1422987]\n",
      "Training accuracy : 0.5645199156383011\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4798698329881935\n",
      "\n",
      "Statistics for epoch 29\n",
      "Training loss : [1.7571877 1.6416224 1.3310524 ... 1.156421  1.147537  1.1427231]\n",
      "Training accuracy : 0.5645690444243974\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4799535032437165\n",
      "\n",
      "Statistics for epoch 30\n",
      "Training loss : [1.756793  1.6412998 1.330679  ... 1.1566962 1.1477141 1.1429814]\n",
      "Training accuracy : 0.5646208917994476\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4799998207065953\n",
      "\n",
      "Statistics for epoch 31\n",
      "Training loss : [1.7564664 1.6410881 1.3304422 ... 1.1600277 1.1508027 1.147735 ]\n",
      "Training accuracy : 0.564254335388824\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4802060081219912\n",
      "\n",
      "Statistics for epoch 32\n",
      "Training loss : [1.7547529 1.6395258 1.328088  ... 1.1615078 1.1522174 1.1494299]\n",
      "Training accuracy : 0.564370393245834\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.480335995840393\n",
      "\n",
      "Statistics for epoch 33\n",
      "Training loss : [1.7544153 1.639393  1.3276782 ... 1.1622506 1.1528995 1.1501832]\n",
      "Training accuracy : 0.564411884091536\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4803673721862141\n",
      "\n",
      "Statistics for epoch 34\n",
      "Training loss : [1.7542422 1.6393266 1.3275021 ... 1.1626875 1.15328   1.1505989]\n",
      "Training accuracy : 0.5644343448145602\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4803524310691564\n",
      "\n",
      "Statistics for epoch 35\n",
      "Training loss : [1.7540996 1.6392415 1.3273674 ... 1.1629777 1.1535242 1.1508642]\n",
      "Training accuracy : 0.5644560935261917\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.48035392518086223\n",
      "\n",
      "Statistics for epoch 36\n",
      "Training loss : [1.7539707 1.6391736 1.3272667 ... 1.1634543 1.1540442 1.151873 ]\n",
      "Training accuracy : 0.5644150557786488\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.48034346639892184\n",
      "\n",
      "Statistics for epoch 37\n",
      "Training loss : [1.7536297 1.6388909 1.3267231 ... 1.1638767 1.154477  1.152555 ]\n",
      "Training accuracy : 0.564435833565654\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.480335995840393\n",
      "\n",
      "Statistics for epoch 38\n",
      "Training loss : [1.7534547 1.6387602 1.3264377 ... 1.1641862 1.1547892 1.1530043]\n",
      "Training accuracy : 0.564450073793508\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4803584075159795\n",
      "\n",
      "Statistics for epoch 39\n",
      "Training loss : [1.7533437 1.6386914 1.3262713 ... 1.164429  1.1550325 1.1533206]\n",
      "Training accuracy : 0.5644581648320615\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4803464546223334\n",
      "\n",
      "Statistics for epoch 40\n",
      "Training loss : [1.753265  1.6386495 1.3261658 ... 1.1646249 1.1552262 1.1535544]\n",
      "Training accuracy : 0.5644669031536991\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.48033898406380454\n",
      "\n"
     ]
    }
   ],
   "source": [
    "name = 'id2'\n",
    "layers = [410, 400, 400, 400, 400, 138]\n",
    "batch_norm = True\n",
    "epochs = 40\n",
    "lrate = 3e-1\n",
    "batch_size = 5000\n",
    "decay = True\n",
    "\n",
    "net = generate_net(layers, batch_norm)\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, lrate, batch_size=batch_size, decay=True, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Statistics for epoch 1\n",
      "Training loss : [4.949481  4.782579  4.615255  ... 1.3419937 1.3199406 1.3531142]\n",
      "Training accuracy : 0.44084903863250835\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.3827316545494207\n",
      "\n",
      "Statistics for epoch 2\n",
      "Training loss : [2.4268448 2.3098204 1.9035733 ... 1.2247621 1.2167226 1.2491956]\n",
      "Training accuracy : 0.511005009906344\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4033459137538959\n",
      "\n",
      "Statistics for epoch 3\n",
      "Training loss : [2.2400296 2.1146753 1.7216992 ... 1.1656901 1.1667331 1.1914376]\n",
      "Training accuracy : 0.5311135062023636\n",
      "Validation loss : 0.0\n",
      "Validation accuracy : 0.4159771341144549\n",
      "\n",
      "Epoch    4 Batch    720 (23.30%)"
     ]
    }
   ],
   "source": [
    "name = 'id3'\n",
    "layers = [410, 400, 400, 400, 400, 400, 138]\n",
    "batch_norm = True\n",
    "epochs = 40\n",
    "lrate = 3e-1\n",
    "batch_size = 5000\n",
    "decay = True\n",
    "\n",
    "net = generate_net(layers, batch_norm)\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, lrate, batch_size=batch_size, decay=True, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'id4'\n",
    "layers = [410, 2000, 1000, 1000, 500, 500, 250, 138]\n",
    "batch_norm = True\n",
    "epochs = 40\n",
    "lrate = 3e-1\n",
    "batch_size = 5000\n",
    "decay = True\n",
    "\n",
    "net = generate_net(layers, batch_norm)\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, lrate, batch_size=batch_size, decay=True, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'id6'\n",
    "layers = [410, 400, 400, 400, 138]\n",
    "batch_norm = True\n",
    "epochs = 40\n",
    "lrate = 3e-1\n",
    "batch_size = 500\n",
    "decay = True\n",
    "\n",
    "net = generate_net(layers, batch_norm)\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, lrate, batch_size=batch_size, decay=True, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'id7'\n",
    "layers = [410, 800, 800, 400, 400, 400, 200, 200, 138]\n",
    "batch_norm = True\n",
    "epochs = 40\n",
    "lrate = 3e-1\n",
    "batch_size = 5000\n",
    "decay = True\n",
    "\n",
    "net = generate_net(layers, batch_norm)\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, lrate, batch_size=batch_size, decay=True, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'id8'\n",
    "layers = [410, 800, 800, 400, 400, 400, 200, 200, 138]\n",
    "batch_norm = True\n",
    "epochs = 40\n",
    "lrate = 3e-1\n",
    "batch_size = 64\n",
    "decay = True\n",
    "\n",
    "net = generate_net(layers, batch_norm)\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, lrate, batch_size=batch_size, decay=True, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(210, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 138)\n",
    ")\n",
    "name = 'cont10_400x400x400_1e-2_bn'\n",
    "epochs = 20\n",
    "learning_rate = 1e-2\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, learning_rate, batch_size=5000, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(210, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 138)\n",
    ")\n",
    "name = 'cont10_400x400x400_1e-3_bn'\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, learning_rate, batch_size=5000, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(210, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 138)\n",
    ")\n",
    "name = 'cont10_400x400x400_1e-4_bn'\n",
    "epochs = 20\n",
    "learning_rate = 1e-4\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, learning_rate, batch_size=5000, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(210, 800),\n",
    "    nn.BatchNorm1d(800),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(800, 600),\n",
    "    nn.BatchNorm1d(600),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(600, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 200),\n",
    "    nn.BatchNorm1d(200),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(200, 138)\n",
    ")\n",
    "name = 'cont10_800x600x400x200_1e-2_bn'\n",
    "epochs = 20\n",
    "learning_rate = 1e-2\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, learning_rate, batch_size=5000, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(210, 800),\n",
    "    nn.BatchNorm1d(800),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(800, 600),\n",
    "    nn.BatchNorm1d(600),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(600, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 200),\n",
    "    nn.BatchNorm1d(200),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(200, 138)\n",
    ")\n",
    "name = 'cont10_800x600x400x200_5e-2_bn'\n",
    "epochs = 20\n",
    "learning_rate = 5e-2\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, learning_rate, batch_size=5000, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(210, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 138)\n",
    ")\n",
    "name = 'cont10_400x400x400_5e-2_bn'\n",
    "epochs = 20\n",
    "learning_rate = 5e-2\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, learning_rate, batch_size=5000, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential(\n",
    "    nn.Linear(210, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 400),\n",
    "    nn.BatchNorm1d(400),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(400, 138)\n",
    ")\n",
    "name = 'cont10_400x400x400_bn_dlr'\n",
    "epochs = 20\n",
    "learning_rate = 0.1\n",
    "training_routine(name, net, (trainx, trainy, valx, valy), epochs, learning_rate, batch_size=5000, logging=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
