{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import optparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "trainx = np.load('data/train.npy', encoding='bytes')\n",
    "trainy = np.load('data/train_labels.npy', encoding='bytes')\n",
    "testx = np.load('data/test.npy', encoding='bytes')\n",
    "valx = np.load('data/dev.npy', encoding='bytes')\n",
    "valy = np.load('data/dev_labels.npy', encoding='bytes')\n",
    "\n",
    "# Preprocessing data\n",
    "trainx = np.concatenate(trainx.tolist())\n",
    "trainy = np.concatenate(trainy.tolist())\n",
    "testx = np.concatenate(testx.tolist())\n",
    "valx = np.concatenate(valx.tolist())\n",
    "valy = np.concatenate(valy.tolist())\n",
    "\n",
    "# Turn into tensors\n",
    "trainx = torch.from_numpy(trainx).float()\n",
    "trainy = torch.from_numpy(trainy.astype(int))\n",
    "testx = torch.from_numpy(testx).float()\n",
    "valx = torch.from_numpy(valx).float()\n",
    "valy = torch.from_numpy(valy.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging for Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger(object):\n",
    "    \"\"\"Logging in tensorboard without tensorflow ops.\"\"\"\n",
    "\n",
    "    def __init__(self, log_dir):\n",
    "        self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def log_scalar(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\n",
    "        Parameter\n",
    "        ----------\n",
    "        tag : Name of the scalar\n",
    "        value : value itself\n",
    "        step :  training iteration\n",
    "        \"\"\"\n",
    "        # Notice we're using the Summary \"class\" instead of the \"tf.summary\" public API.\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "\n",
    "    def log_histogram(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Logs the histogram of a list/vector of values.\"\"\"\n",
    "        # Convert to a numpy array\n",
    "        values = np.array(values)\n",
    "        \n",
    "        # Create histogram using numpy        \n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill fields of histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Requires equal number as bins, where the first goes from -DBL_MAX to bin_edges[1]\n",
    "        # See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/summary.proto#L30\n",
    "        # Thus, we drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_routine(net, \n",
    "                     dataset,\n",
    "                     n_iters, \n",
    "                     criterion=nn.CrossEntropyLoss(), \n",
    "                     batch_size=5000,\n",
    "                     optim=None,\n",
    "                     stats_freq=10, \n",
    "                     lr=0.1,\n",
    "                     batch_val=False,\n",
    "                     val_acc_logger=None,\n",
    "                     train_acc_logger=None):\n",
    "\n",
    "    # loggers for tensorboard visualization\n",
    "    if val_acc_logger:\n",
    "        vLog = Logger('./logs/{}'.format(val_acc_logger))\n",
    "    if train_acc_logger:\n",
    "        tLog = Logger('./logs/{}'.format(train_acc_logger))\n",
    "    \n",
    "    # organize the data\n",
    "    train_data, train_labels, val_data, val_labels = dataset\n",
    "\n",
    "    if not optim:\n",
    "        optimizer=torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = optim\n",
    "\n",
    "    # GPU\n",
    "    gpu = torch.cuda.is_available()\n",
    "    if gpu:\n",
    "        print('Using GPU')\n",
    "        net = net.cuda()\n",
    "    else:\n",
    "        print('Not using GPU')\n",
    "\n",
    "    # training\n",
    "    for i in range(n_iters):\n",
    "\n",
    "        train_output = []\n",
    "        train_loss = []\n",
    "\n",
    "        for j in range(0, train_data.shape[0], batch_size):\n",
    "            # print progress\n",
    "            if j % (batch_size * 50) == 0:\n",
    "                print('\\rEpoch {:4} Batch {:6} ({:.2%})'.format(i+1, j // batch_size, j / train_data.shape[0]), end='')\n",
    "            \n",
    "            # create batches\n",
    "            b_train_data, b_train_labels = train_data[j : j + batch_size], train_labels[j : j + batch_size]\n",
    "\n",
    "            # use gpu if possible\n",
    "            if gpu:\n",
    "                b_train_data, b_train_labels = b_train_data.cuda(), b_train_labels.cuda()\n",
    "\n",
    "            # forward pass\n",
    "            b_train_output = net(b_train_data)\n",
    "            b_train_loss = criterion(b_train_output, b_train_labels)\n",
    "            \n",
    "            # backward pass and optimization\n",
    "            b_train_loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # save output\n",
    "            if len(b_train_output.shape) != 0:\n",
    "                x = b_train_output\n",
    "                x = x.cpu().detach()\n",
    "                train_output.append(x)\n",
    "            if b_train_loss:\n",
    "                train_loss.append(b_train_loss.cpu().detach())\n",
    "        \n",
    "        if train_acc_logger:\n",
    "            train_output = torch.cat(train_output)\n",
    "            train_loss = torch.FloatTensor(train_loss)\n",
    "            train_prediction = train_output.cpu().detach().argmax(dim=1)\n",
    "            train_accuracy = (train_prediction.numpy()==train_labels.numpy()).mean()\n",
    "\n",
    "            # saving training accuracy for tensorboard\n",
    "            tr_info = { 'accuracy': train_accuracy }\n",
    "            for tag, value in tr_info.items():\n",
    "                tLog.log_scalar(tag, value, i+1)\n",
    "            \n",
    "#             # 2. Log values and gradients of the parameters (histogram summary)\n",
    "#             for tag, value in net.named_parameters():\n",
    "#                 tag = tag.replace('.', '/')\n",
    "#                 tLog.log_histogram(tag, value.data.cpu().numpy(), i+1)\n",
    "#                 tLog.log_histogram(tag+'/grad', value.grad.data.cpu().numpy(), i+1)\n",
    "        \n",
    "\n",
    "        # Once every 10 iterations, print statistics\n",
    "        if (i+1) % stats_freq == 0 or i == 0:\n",
    "            # override carriage return\n",
    "            print(\"\\rStatistics for epoch\", i+1)\n",
    "\n",
    "            # computing overall output and loss\n",
    "            if not train_acc_logger:\n",
    "                # we will have done it already, if training logging is active\n",
    "                train_output = torch.cat(train_output)\n",
    "                train_loss = torch.FloatTensor(train_loss)\n",
    "                # compute the accuracy of the prediction\n",
    "                train_prediction = train_output.cpu().detach().argmax(dim=1)\n",
    "                train_accuracy = (train_prediction.numpy()==train_labels.numpy()).mean()\n",
    "            \n",
    "            if batch_val:\n",
    "                total_val_loss = 0\n",
    "                total_val_accuracy = 0\n",
    "                for j in range(0, val_data.shape[0], batch_size):\n",
    "                    # create batches\n",
    "                    val_data_b = val_data[j : j + batch_size]\n",
    "                    val_labels_b = val_labels[j : j + batch_size]\n",
    "                    # use GPU if possible \n",
    "                    if gpu:\n",
    "                        val_data_b, val_labels_b = val_data_b.cuda(), val_labels_b.cuda()\n",
    "                    # Now for the validation set\n",
    "                    val_output = net(val_data_b)\n",
    "                    val_loss = criterion(val_output, val_labels_b)\n",
    "                    # compute the accuracy of the prediction\n",
    "                    val_prediction = val_output.cpu().detach().argmax(dim=1)\n",
    "                    val_accuracy = (val_prediction.numpy() == val_labels_b.cpu().detach().numpy()).mean()\n",
    "                    # sum up to get mean later\n",
    "                    total_val_loss += val_loss\n",
    "                    total_val_accuracy += val_accuracy\n",
    "                # compute mean validation loss and accuracy for all batches\n",
    "                val_loss = total_val_loss / (val_data.shape[0] // j)\n",
    "                val_accuracy = total_val_accuracy / (val_data.shape[0] // j)\n",
    "                \n",
    "            else:\n",
    "                # use GPU if possible \n",
    "                if gpu:\n",
    "                    val_data, val_labels = val_data.cuda(), val_labels.cuda()\n",
    "                # Now for the validation set\n",
    "                val_output = net(val_data)\n",
    "                val_loss = criterion(val_output, val_labels)\n",
    "                # compute accuracy\n",
    "                val_prediction = val_output.cpu().detach().argmax(dim=1)\n",
    "                val_accuracy = (val_prediction.numpy() == val_labels.cpu().detach().numpy()).mean()\n",
    "                \n",
    "            \n",
    "            print(\"Training loss :\",train_loss.cpu().detach().numpy())\n",
    "            print(\"Training accuracy :\",train_accuracy)\n",
    "            print(\"Validation loss :\",val_loss.cpu().detach().numpy())\n",
    "            print(\"Validation accuracy :\",val_accuracy)\n",
    "            print()\n",
    "            \n",
    "            if val_acc_logger:\n",
    "                tr_info = { 'accuracy': val_accuracy }\n",
    "                for tag, value in tr_info.items():\n",
    "                    vLog.log_scalar(tag, value, i+1) \n",
    "\n",
    "            # try to save the neural network\n",
    "            try:\n",
    "                torch.save(net, 'neural_net')\n",
    "            except:\n",
    "                print('Could not save neural network.')\n",
    "\n",
    "    net = net.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NN\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(40, 200),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(200, 138)\n",
    ")\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 10000\n",
    "model_name = 'small_lrelu_momtm05'\n",
    "optim = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NN\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(40, 200),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(200, 138)\n",
    ")\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 10000\n",
    "model_name = 'small_lrelu_momtm05_lrdecay'\n",
    "optim = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.5)\n",
    "torch.optim.lr_scheduler.ReduceLROnPlateau(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NN\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(40, 200),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(200, 138)\n",
    ")\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 10000\n",
    "model_name = 'small_tanh'\n",
    "optim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NN\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(40, 200),\n",
    "    nn.LeakyReLU(0.3),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.LeakyReLU(0.3),\n",
    "    nn.Linear(200, 138)\n",
    ")\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 10000\n",
    "model_name = 'small_lrelu03'\n",
    "optim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NN\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(40, 200),\n",
    "    nn.LeakyReLU(0.05),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.LeakyReLU(0.05),\n",
    "    nn.Linear(200, 138)\n",
    ")\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 10000\n",
    "model_name = 'small_lrelu005'\n",
    "optim = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating NN\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(40, 200),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(200, 200),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(200, 138)\n",
    ")\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "batch_size = 10000\n",
    "model_name = 'small_tanh'\n",
    "optim = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n",
      "Statistics for epoch 1\n",
      "Training loss : [6.199172  9.244638  7.620674  ... 2.9390204 2.905887  2.5937262]\n",
      "Training accuracy : 0.21786150485161326\n",
      "Validation loss : 3.254351\n",
      "Validation accuracy : 0.20562413528285028\n",
      "\n",
      "Statistics for epoch 10\n",
      "Training loss : [3.3608332 2.8489623 3.0729864 ... 2.6450016 2.635248  2.2794402]\n",
      "Training accuracy : 0.29015752345867174\n",
      "Validation loss : 3.0705924\n",
      "Validation accuracy : 0.23563187478148617\n",
      "\n",
      "Epoch   19 Batch   1500 (97.09%)"
     ]
    }
   ],
   "source": [
    "training_routine(net, \n",
    "                 (trainx, trainy, valx, valy), \n",
    "                 epochs, \n",
    "                 lr=learning_rate, \n",
    "                 batch_size=batch_size, \n",
    "                 optim=optim,\n",
    "                 train_acc_logger='train_acc_{}'.format(model_name),\n",
    "                 val_acc_logger='val_acc_{}'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    torch.save(net, 'neural_net_{}'.format(model_name))\n",
    "except:\n",
    "    print('Could not save neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
